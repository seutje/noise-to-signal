# noise-to-signal — GAN Album Visualizer (Design Doc)

**Author:**  
**Date:** October 31, 2025  
**Status:** Draft v3.0 (album playlist edition; offline prerender pipeline)

---

## 1) Product Goal

A **Python-based rendering toolkit** that converts a **predefined album** (a fixed set of WAV/MP3 tracks bundled with the project) into **prerendered video**. Each track produces abstract, “dream-like” visuals generated by driving trajectories through a compact latent **GAN generator** trained on abstract imagery. Videos are rendered offline (no live playback requirement) and encoded to MP4/WebM for distribution.

**Project name:** `noise-to-signal`

**Key outcomes**
- **Deterministic offline renders**: given audio, seeds, and configuration, rerun produces identical frames.  
- **Album-first workflow:** batch-render the entire playlist; produce per-track MP4 plus stitched “album film”.  
- **High visual fidelity:** 30–60 fps output at 1080p with motion-smoothing and post-processing handled offline.  
- **Python runtime only:** CLI orchestrated by Python scripts; leverages PyTorch for inference, `librosa`, and FFmpeg.
- **All assets in repo:** training **data images**, final **model checkpoint** (PyTorch/optional ONNX), and album audio (via **Git LFS**).
- **Automated tests:** **Pytest** covering feature extraction, latent controller, and render scheduling.  
- **Dataset generation:** Local script using **Stable Diffusion 1.5** (SD 1.5) to synthesize training images.

> **Non-goals:** Browser-based live rendering, WebGL/WebGPU pipelines, or JavaScript UI frameworks. Development-time tooling (PyTorch, diffusers, FFmpeg, Pytest) is allowed.

---

## 2) System Overview

```
[ Prompts + Seeds ] --(SD 1.5 / diffusers)--> [ 5k Training Images (+ JSON meta) ]

[ 5k Images ] --(augment)--> [ GAN Training (PyTorch/Lightning) ]
                              | losses: logistic GAN + chroma/saturation regs
                              v
                       [ Generator checkpoint (fp32) + meta.json ]

[ Predefined Album ]
  /album/*.mp3|*.wav + tracklist.json (titles, order, bpm, cues)

[ Offline Renderer (Python CLI) ]
  - Analyse audio with librosa/torchaudio → per-frame features
  - Latent controller synthesizes trajectories per track
  - PyTorch (CPU/GPU) decodes frames to numpy arrays
  - Post-processing (filmic tone map, grain, overlays)
  - FFmpeg encodes frames + audio → MP4/WebM
  - Playlist stitcher merges segments into full-album video
```

---

## 3) Requirements

### 3.1 Functional
- **Track rendering:** Convert each album track into a synchronized 1920×1080 (configurable) video with audio.  
- **Batch mode:** CLI command `python render_album.py --config render.yaml` renders every track sequentially and exports both per-track videos and a stitched feature-length cut.  
- **Feature extraction:** Analyse audio offline (RMS, spectral centroid, flatness, MFCC-lite, onset strength) at fixed hop sizes and cache to `/cache/features/*.npy`.  
- **Latent presets:** Support preset configurations (“moods”) that define latent anchor weights, smoothing constants, color grading, and shuffle seeds.  
- **Deterministic reruns:** Persist seeds, controller parameters, and render metadata to `/renders/<run-id>/meta.json`.  
- **Preview frames:** Export keyframes (PNG) and GIF snippets to `/renders/<run-id>/previews/` for quick validation.

### 3.2 Non-functional
- **Offline execution:** No browser dependencies; runtime is Python 3.10+ with GPU acceleration optional.  
- **Performance:** 60 fps render target with ability to oversample (e.g., render 120 fps and decimate) to avoid banding; pipeline must render a 5-minute track ≤ 15 minutes on RTX 4070 (or document timing).  
- **Resource management:** Stream frames to FFmpeg (pipe) to avoid excessive disk usage; configurable chunked renders for low-memory systems.  
- **Reproducibility:** All randomization seeded; configuration files committed; metadata logged in `DEVLOG.md`.  
- **Testing:** Pytest coverage ≥85% statements/branches across feature extraction, controller math, and render scheduling.

---

## 4) Dataset Generation (Local) — Stable Diffusion 1.5

> **Licensing:** Generated images are generally redistributable; do **not** commit SD 1.5 weights unless permitted. Commit the generated dataset and scripts; fetch weights locally during generation. You must own the rights to the album MP3s or use permissive licenses; document them in `AUDIO_LICENSE.md`.

### 4.1 Environment
Python ≥ 3.10, CUDA 12.x, PyTorch ≥ 2.2, `diffusers`, `transformers`, `safetensors`, `accelerate` (optional `xformers`).

### 4.2 Prompts
Buckets covering abstract, glitch, color-field, reaction–diffusion, monochrome noise; negative prompts exclude text/logos/people/objects. Steps 20–35, CFG 6.5–9.5, 512×512.

### 4.3 Script
`/training/sd_generate.py` produces `PNG` + sidecar JSON + `manifest.csv`. Optional CLIP/aesthetic culling of the worst 10–20%.

### 4.4 Augmentation
Dataloader-time: random crop/resize, color jitter, mild blur/JPEG, small cutout.

---

## 5) Model & Training (GAN)

### 5.1 Architecture
- Train at **256** first; optional **512** pass.
- Latent **z** shape: **(8, 16, 16)** with spatial noise to preserve controller compatibility.
- Generator: ResBlocks + Upsample stack (GroupNorm + SiLU, `tanh` output).
- Discriminator: mirrored ResBlocks with strided downsamples and global average head.

### 5.2 Loss & Optim
- Adversarial: non-saturating logistic GAN objective (softplus) with R1 gradient penalty (γ≈10, applied every 16 steps).
- Colour regularisation: batch-level chroma alignment + saturation preservation terms.
- Optimisers: AdamW (β=(0.5, 0.999)), LR≈2e-4 for both nets, cosine schedule, EMA of generator (0.999).
- Precision: default full fp32; optional gradient checkpointing for generator blocks to limit VRAM usage.

### 5.3 Validation
Sample grids from fixed noise, LPIPS vs. held-out real batch, latent interpolation sweeps, saturation/chroma statistics.

---

## 6) Export & Metadata

- Primary artefact is the full-precision Lightning checkpoint (`training/outputs/checkpoints/gan-*.ckpt`).
- Optional: export the **generator** to FP32 ONNX via `/training/export_onnx.py` (opset ≥ 18, dynamic shapes off by default).
- `models/meta.json` stores latent shape, normalisation params, version, checkpoint reference, precomputed **anchor latents**, and optional **PCA axes**.

---

## 7) Renderer Toolkit (Python CLI)

### 7.1 File Layout
```
/album
  tracklist.json          # [{ "src":"01.wav","title":"…","artist":"…","bpm":120, "keys":[...]}]
  stems/                  # optional per-track stems for advanced features
  01-opening.wav …        # album files (LFS)
/renderer
  render_album.py         # orchestrates full playlist renders
  render_track.py         # renders a single track given config + cached features
  audio_features.py       # librosa/torchaudio feature extraction + caching
  controller.py           # latent anchoring, wander, preset blending
  decoder.py              # ONNX Runtime session wrapper (CPU/CUDA)
  postfx.py               # tone mapping, color grading, overlays, grain
  frame_writer.py         # numpy frame → FFmpeg pipe / image sequence
  config_schema.py        # pydantic schema for render.yaml validation
  presets/
    default.yaml
    pulse.yaml
    drift.yaml
/models
  generator.fp32.onnx     # optional export (LFS)
  meta.json
/training
  sd_generate.py
  prompts.yaml
  train_gan.py
  models.py
  export_onnx.py
/data
  sd15_abstract/*.png     # generated images (LFS)
  manifest.csv
/tests
  test_audio_features.py
  test_controller.py
  test_render_schedule.py
/.github/workflows/ci.yml
AUDIO_LICENSE.md
DATA_LICENSE.md
RENDER_GUIDE.md           # how to run renders & manage outputs
```

### 7.2 Render Flow
1. **Bootstrap:** Load `render.yaml`, `tracklist.json`, and `models/meta.json`. Resolve preset overlays and seeds.  
2. **Feature Extraction:** For each track, compute features with `librosa.load` (or torchaudio) at a fixed sample rate; cache to `/cache/features/<track>.npz`.  
3. **Latent Trajectory:** `controller.py` consumes the feature timeline (resampled to target FPS) and outputs latent tensors `(frames × 8 × 16 × 16)` using anchor interpolation + wander noise.  
4. **Decoding:** `decoder.py` batches frames (e.g., 32 at a time) through ONNX Runtime. Supports CUDAExecutionProvider when available; otherwise CPU.  
5. **PostFX:** `postfx.py` applies color grading LUTs, vignette, grain, and optional trailing persistence using numpy/CuPy.  
6. **Encoding:** `frame_writer.py` streams frames to FFmpeg via stdin (`ffmpeg -y -f rawvideo ...`) while muxing original audio. Intermediate image sequences optional when `--debug-frames` enabled.  
7. **Stitching:** `render_album.py` optionally concatenates per-track MP4s with FFmpeg `concat` demuxer to create `/renders/<run-id>/album.mp4`.

### 7.3 Render Configuration (`render.yaml`)
```yaml
output_root: renders/2025-11-07_album-v1
frame_rate: 60
resolution: [1920, 1080]
audio:
  sample_rate: 48000
  normalization: -14
controller:
  preset: pulse
  smoothing_alpha: 0.92
  wander_seed: 42
  tempo_sync:
    enabled: true
    subdivision: 0.5
decoder:
  batch_size: 32
  execution_provider: cuda
postfx:
  tone_curve: filmlog
  grain_intensity: 0.15
  chroma_shift: 0.004
tracks:
  - id: opening
    src: 01-opening.wav
    preset: drift
    trim:
      start: 2.0
      end: null
  - id: interference
    src: 02-interference.wav
    preset: pulse
```

`config_schema.py` validates ranges (frame rate 24–120, resolution divisible by 16, etc.). CLI supports overrides (`--preset`, `--fps`, `--resume`).

### 7.4 Performance Notes
- Prefer CUDAExecutionProvider when available; fallback to CPU with multi-process chunking.  
- Use sliding-window caching to reuse decoded frame batches for post-processing steps.  
- Support checkpoint/resume by persisting last encoded frame index and random state.  
- Encapsulate FFmpeg command templates to auto-tune bitrate (`-crf`, `-preset`) based on resolution and expected detail.  
- Provide profiling utilities (`python renderer/profile.py`) to log render throughput and detect bottlenecks.

---

## 8) Automated Tests (Pytest)

> GPU acceleration is optional; tests run deterministically on CPU with mocks for ONNX Runtime and FFmpeg.

### 8.1 Unit Tests
- `test_prompts.py` — verifies prompt buckets, CFG ranges, seed determinism.  
- `test_audio_features.py` — synthetic sine/noise fixtures → RMS, centroid, flatness, MFCC-lite correctness; caching integrity.  
- `test_controller.py` — latent anchors sum to one, wander noise obeys bounds, tempo sync aligns with beat grid.  
- `test_decoder_batcher.py` — ONNX mock ensures batching splits/merges correctly; validates handling of dropped frames on failure.  
- `test_postfx.py` — tone curve LUTs and grain overlays deterministic under fixed seed.

### 8.2 Integration Tests
- `test_render_track.py` — runs minimal 5-second fixture track end-to-end (features → latent → frames → FFmpeg stub) and asserts metadata output.  
- `test_render_album.py` — multi-track orchestration, resume-from-checkpoint logic, preview generation.

### 8.3 Mocks & Fixtures
- **ONNX Runtime mock**: inject deterministic tensor outputs shaped like real decoder frames.  
- **FFmpeg stub**: replaces subprocess call with in-memory buffer accounting; validates command arguments.  
- **Audio fixtures**: use generated sweeps/beats with known tempo to test tempo synchronization.

### 8.4 Coverage & CI
- Target **≥85%** coverage.  
- GitHub Actions workflow runs lint (`ruff`/`black --check`) + pytest + coverage artifact (XML + HTML). LFS checkout enabled for models/data/audio.

---

## 9) Repository & Size Management

- Use **Git LFS** for `*.onnx`, `data/*.png`, `album/*.mp3`, and rendered `*.mp4`/`*.webm` previews.  
- `AUDIO_LICENSE.md` documents rights to the included MP3s (creator, license, attribution if required).  
- `DATA_LICENSE.md` documents generated-image licensing.  
- Provide `CONTRIBUTING.md` with guidance for partial LFS clone to reduce bandwidth.

---

## 10) Security & Privacy

- No microphone or external services required; renderer consumes bundled audio only.  
- All processing occurs locally; scripts must not reach out to remote APIs (no telemetry, no dataset downloads at render time).  
- Store cached features and renders under project directories to avoid leaking to shared temp folders; document cleanup steps.  
- When distributing rendered videos, ensure license metadata accompanies files.

---

## 11) Risk Log & Mitigations

| Risk | Impact | Mitigation |
|---|---|---|
| FFmpeg version mismatch | Render failures or poor encode quality | Bundle command templates per platform; document minimum FFmpeg version (≥6.0); add smoke test in CI with stub binary. |
| GPU memory exhaustion | Decoder crashes on high resolutions | Allow CPU fallback and configurable batch sizes; expose `--batch-size` in config; detect and log provider switches. |
| Audio/video drift | Desynced renders | Use timecode derived from audio hop size; resample features to exact frame count; validate muxed duration matches source audio ±5 ms. |
| Disk usage explosion | Renders exceed storage | Stream to FFmpeg pipes by default; offer `--keep-frames` only for debugging; provide cleanup scripts. |
| Large repo | Slow clones | LFS + partial clone docs; archive older render outputs under `/archive/`; prefer compressed previews. |
| Audio rights | Legal | Only include tracks you own or with permissive licenses; document in `AUDIO_LICENSE.md`. |

---

## 12) Milestones

1. **Week 1** — SD dataset (5k), GAN@256 prototype, initial PyTorch checkpoint export, feature extraction research.
2. **Week 2** — Finalize GAN training (fp32 baseline + EMA), implement audio feature cache + latent controller presets.
3. **Week 3** — Build renderer CLI (decoder batching, FFmpeg integration, postfx), generate first full-track render.  
4. **Week 4** — Polish presets, add resume/checkpointing, achieve Pytest coverage target, finalize docs and packaging.

---

## 13) Appendices

### A) `tracklist.json` (full example)
```json
[
  {
    "id": "opening",
    "src": "01-opening.wav",
    "title": "Opening",
    "artist": "noise-to-signal",
    "bpm": 120,
    "key": "Am",
    "art": "cover.png",
    "markers": [15.2, 42.7, 75.0]
  },
  {
    "id": "interference",
    "src": "02-interference.wav",
    "title": "Interference",
    "artist": "noise-to-signal",
    "bpm": 98,
    "key": "F#m"
  },
  {
    "id": "signal",
    "src": "03-signal.wav",
    "title": "Signal",
    "artist": "noise-to-signal",
    "bpm": 132,
    "key": "C#m",
    "loop": false
  }
]
```

### B) Export (CLI)
```bash
# Export generator to ONNX (optional)
python training/export_onnx.py --checkpoint ckpt.pt --onnx-name models/generator.fp32.onnx --opset 18
```

### C) `render_track.py` (core loop sketch)
```python
from pathlib import Path
import numpy as np

from renderer.audio_features import FeatureCache
from renderer.controller import LatentController
from renderer.decoder import DecoderSession
from renderer.frame_writer import FFMpegWriter
from renderer.postfx import apply_postfx

def render_track(track_cfg, run_cfg, meta):
    features = FeatureCache(run_cfg.cache_dir).load_or_compute(track_cfg, run_cfg)
    controller = LatentController(meta, run_cfg.controller, seed=track_cfg.seed or run_cfg.seed)
    decoder = DecoderSession(meta, provider=run_cfg.decoder.provider, batch_size=run_cfg.decoder.batch_size)

    frame_writer = FFMpegWriter(
        output_path=Path(run_cfg.output_root) / f"{track_cfg.id}.mp4",
        frame_rate=run_cfg.frame_rate,
        resolution=run_cfg.resolution,
        audio_src=Path(run_cfg.album_root) / track_cfg.src,
        ffmpeg_path=run_cfg.ffmpeg_path,
    )

    latent_stream = controller.generate(features, run_cfg.frame_rate)
    for batch in latent_stream.batched(decoder.batch_size):
        decoded = decoder.decode(batch)  # np.ndarray [B, H, W, 3]
        graded = apply_postfx(decoded, run_cfg.postfx, seed=controller.seed)
        frame_writer.write(graded)

    frame_writer.close()
    return {
        "track_id": track_cfg.id,
        "frames": frame_writer.frames_written,
        "duration_sec": frame_writer.duration,
        "seed": controller.seed,
    }
```

### D) Test Doubles
- **ONNX Runtime**: inject fake `DecoderSession` that returns deterministic gradients (`np.linspace`) for repeatability.  
- **FFmpeg**: replace subprocess with `DummyWriter` capturing arguments and byte counts; assert mux command structure.  
- **Feature cache**: point to temp directory and pre-seed `.npz` fixtures to test cache hits/misses.

---

## 14) Acceptance Criteria

- `python renderer/render_album.py --config render.yaml` renders the full playlist without errors, producing per-track MP4s and combined album video.  
- Render metadata (`meta.json`, logs, seeds) stored under `/renders/<run-id>/` enabling deterministic reruns.  
- Preview frames/GIFs generated for each track.  
- `pytest` passes with ≥85% coverage; CI pipeline executes lint + tests + coverage upload.  
- Repo includes `/models` (ONNX + meta), `/album` (audio + tracklist), and `/renders` previews via **Git LFS**.  
- LICENSE, `DATA_LICENSE.md`, and `AUDIO_LICENSE.md` included.  
- Renderer operates fully offline (no network access required during render).

---

*End of DESIGN.md.*
